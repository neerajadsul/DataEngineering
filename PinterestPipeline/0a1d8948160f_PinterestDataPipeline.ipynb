{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d4660950-e295-4637-9176-4490759c3409",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# pyspark functions\n",
    "from pyspark.sql.functions import *\n",
    "# URL processing\n",
    "import urllib\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "981e5318-71c4-4ae4-a7fd-5a23fc26ec3f",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "## Read the credentials and prepare to connect to S3 Bucket"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "0e137426-3394-40ca-b4db-3214f38bde6b",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Specify file type to be csv\n",
    "file_type = \"csv\"\n",
    "# Indicates file has first row as the header\n",
    "first_row_is_header = \"true\"\n",
    "# Indicates file has comma as the delimeter\n",
    "delimiter = \",\"\n",
    "# Read the CSV file to spark dataframe\n",
    "aws_keys_df = spark.read.format(file_type)\\\n",
    ".option(\"header\", first_row_is_header)\\\n",
    ".option(\"sep\", delimiter)\\\n",
    ".load(\"/FileStore/tables/authentication_credentials.csv\")\n",
    "\n",
    "# Get the AWS access key and secret key from the spark dataframe\n",
    "ACCESS_KEY = aws_keys_df.where(col('User name')=='databricks-user').select('Access key ID').collect()[0]['Access key ID']\n",
    "SECRET_KEY = aws_keys_df.where(col('User name')=='databricks-user').select('Secret access key').collect()[0]['Secret access key']\n",
    "# Encode the secrete key\n",
    "ENCODED_SECRET_KEY = urllib.parse.quote(string=SECRET_KEY, safe=\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "8eac560b-c873-4db0-9767-729c89ea0631",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "## Connect and Mount S3 Bucket to Databricks File System"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "56dce305-dedb-4c41-b17c-838b7f7427dd",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# AWS S3 bucket name\n",
    "AWS_S3_BUCKET = \"user-0a1d8948160f-bucket\"\n",
    "# Mount name for the bucket\n",
    "MOUNT_NAME = \"/mnt/pin_pipe\"\n",
    "# Source url\n",
    "SOURCE_URL = \"s3n://{0}:{1}@{2}\".format(ACCESS_KEY, ENCODED_SECRET_KEY, AWS_S3_BUCKET)\n",
    "# Mount the drive\n",
    "dbutils.fs.mount(SOURCE_URL, MOUNT_NAME)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "423e0143-99a9-4b39-bf94-c29b263435f8",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "pin_data_location = f\"{MOUNT_NAME}/topics/0a1d8948160f.pin/partition=0/\"\n",
    "geo_data_location = f\"{MOUNT_NAME}/topics/0a1d8948160f.geo/partition=0/\"\n",
    "user_data_location = f\"{MOUNT_NAME}/topics/0a1d8948160f.user/partition=0/\"\n",
    "display(dbutils.fs.ls(pin_data_location))\n",
    "display(dbutils.fs.ls(geo_data_location))\n",
    "display(dbutils.fs.ls(user_data_location))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "d55ec8c3-d7c2-439f-857a-44e4eff38084",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "## Read JSON data files for all pin, geo, user"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "e7ab0a8e-5265-4d42-9fd0-fc18d7d4867e",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "def prepare_dataframe(data_location):\n",
    "    data_files = dbutils.fs.ls(data_location)\n",
    "    df = spark.read\\\n",
    "        .format('json')\\\n",
    "        .option(\"inferSchema\", infer_schema)\\\n",
    "        .load([x.path for x in data_files])\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "15e12d23-4370-4aa5-b92f-01543a6db438",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "df_pin = prepare_dataframe(pin_data_location)\n",
    "df_geo = prepare_dataframe(geo_data_location)\n",
    "df_user = prepare_dataframe(user_data_location)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "49abab27-c99a-478d-8eab-a36ba86c03ff",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "display(df_pin)\n",
    "display(df_geo)\n",
    "display(df_user)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "c1460a27-12f0-47af-8d86-55a69bb34b83",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "dashboards": [],
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "0a1d8948160f_PinterestDataPipeline",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
