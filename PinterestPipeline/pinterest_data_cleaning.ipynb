{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "b8dc6125-605d-4acd-88fa-0d0b81c7ca83",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/html": [
       "<style scoped>\n",
       "  .ansiout {\n",
       "    display: block;\n",
       "    unicode-bidi: embed;\n",
       "    white-space: pre-wrap;\n",
       "    word-wrap: break-word;\n",
       "    word-break: break-all;\n",
       "    font-family: \"Menlo\", \"Monaco\", \"Consolas\", \"Ubuntu Mono\", \"Source Code Pro\", monospace;\n",
       "    font-size: 13px;\n",
       "    color: #555;\n",
       "    margin-left: 4px;\n",
       "    line-height: 19px;\n",
       "  }\n",
       "</style>\n",
       "<div class=\"ansiout\"></div>"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "addedWidgets": {},
       "arguments": {},
       "data": "<div class=\"ansiout\"></div>",
       "datasetInfos": [],
       "metadata": {},
       "removedWidgets": [],
       "type": "html"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "def prepare_dataframe(data_location):\n",
    "    \"\"\"Read JSON data files for all pinterest posts data into a Dataframe.\"\"\"\n",
    "    df = spark.read.json(data_location)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "291b1705-1280-4ba1-88ef-338398399668",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/html": [
       "<style scoped>\n",
       "  .ansiout {\n",
       "    display: block;\n",
       "    unicode-bidi: embed;\n",
       "    white-space: pre-wrap;\n",
       "    word-wrap: break-word;\n",
       "    word-break: break-all;\n",
       "    font-family: \"Menlo\", \"Monaco\", \"Consolas\", \"Ubuntu Mono\", \"Source Code Pro\", monospace;\n",
       "    font-size: 13px;\n",
       "    color: #555;\n",
       "    margin-left: 4px;\n",
       "    line-height: 19px;\n",
       "  }\n",
       "</style>\n",
       "<div class=\"ansiout\"></div>"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "addedWidgets": {},
       "arguments": {},
       "data": "<div class=\"ansiout\"></div>",
       "datasetInfos": [],
       "metadata": {},
       "removedWidgets": [],
       "type": "html"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "def transform_followers_count(x) -> int:\n",
    "    \"\"\"Transform k with 1000 and M with 1000000 for follower count.\"\"\"\n",
    "    muliplier = 1\n",
    "    if x.endswith('k'):\n",
    "        return 1000 * int(x[:-1])\n",
    "    elif x.endswith('M'):\n",
    "        return 1000000 * int(x[:-1])\n",
    "    else:\n",
    "        return int(x)\n",
    "\n",
    "\n",
    "def clean_pin_dataframe(df_pin):\n",
    "    \"\"\"\n",
    "    Replace empty entries and entries with no relevant data in each column with `Nones` \\\n",
    "    Perform the necessary transformations on the follower_count to ensure every entry is a number. \\\n",
    "    Make sure the data type of this column is an `int`. \\\n",
    "    Ensure that each column containing numeric data has a numeric data type \\\n",
    "    Clean the data in the `save_location` column to include only the save location path \\\n",
    "    Rename the `index` column to `ind`. \\\n",
    "    Reorder the `DataFrame` columns to have the following column order: \\\n",
    "        `ind` \\\n",
    "        `unique_id` \\\n",
    "        `title` \\\n",
    "        `description` \\\n",
    "        `follower_count` \\\n",
    "        `poster_name` \\\n",
    "        `tag_list` \\\n",
    "        `is_image_or_video` \\\n",
    "        `image_src` \\\n",
    "        `save_location` \\\n",
    "        `category`\n",
    "    \"\"\"\n",
    "    # uuid_regex = r'[a-z0-9]{8}-[a-z0-9]{4}-[a-z0-9]{4}-[a-z0-9]{4}-[a-z0-9]{12}'\n",
    "    # pin_dfs = pin_dfs[pin_dfs['unique_id'].str.match(uuid_regex)]\n",
    "    # len(pin_dfs)\n",
    "    # Make sure unique ids are correct format\n",
    "    df_pin = df_pin.filter(length(df_pin.unique_id) == 36)\n",
    "    ### Cleanup and transform `follower_count`\n",
    "    #### 1. Remove rows with invalid follower count\n",
    "    follower_regex = r'[0-9]{1,}[kM]?'\n",
    "    df_pin = df_pin.filter(df_pin.follower_count.rlike(follower_regex))\n",
    "    #### 2. Convert *kilo* and *Million* to numeric    \n",
    "    transform_followers_udf = udf(transform_followers_count, IntegerType())\n",
    "    df_pin = df_pin.withColumn('follower_count', transform_followers_udf('follower_count'))\n",
    "    ### Rename index column to `ind`\n",
    "    df_pin = df_pin.withColumnRenamed('index', 'ind')\n",
    "    ### Cleanup the `save_location` columns to keep only the path\n",
    "    df_pin = df_pin.withColumn('save_location', regexp_replace('save_location', 'Local save in ', ''))\n",
    "    ### Reorder the columns\n",
    "    df_pin = df_pin[['ind', 'unique_id', 'title', 'description', 'follower_count', 'poster_name',\n",
    "                   'tag_list', 'is_image_or_video', 'image_src', 'save_location', 'category']]\n",
    "    ### Replace empty and not applicable with `None`\n",
    "    df_pin = df_pin.replace(['', 'N/A', 'n/a', 'none', 'None'], None)\n",
    "    ### Finally, drop duplicate rows\n",
    "    df_pin = df_pin.drop_duplicates()\n",
    "\n",
    "    return df_pin\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "bb4c782d-7a29-426a-87c9-f8fabd3f44b8",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/html": [
       "<style scoped>\n",
       "  .ansiout {\n",
       "    display: block;\n",
       "    unicode-bidi: embed;\n",
       "    white-space: pre-wrap;\n",
       "    word-wrap: break-word;\n",
       "    word-break: break-all;\n",
       "    font-family: \"Menlo\", \"Monaco\", \"Consolas\", \"Ubuntu Mono\", \"Source Code Pro\", monospace;\n",
       "    font-size: 13px;\n",
       "    color: #555;\n",
       "    margin-left: 4px;\n",
       "    line-height: 19px;\n",
       "  }\n",
       "</style>\n",
       "<div class=\"ansiout\"></div>"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "addedWidgets": {},
       "arguments": {},
       "data": "<div class=\"ansiout\"></div>",
       "datasetInfos": [],
       "metadata": {},
       "removedWidgets": [],
       "type": "html"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "def clean_geo_dataframe(df_geo):\n",
    "    \"\"\"\n",
    "    Clean Geo Spark DataFrame\n",
    "    To clean the df_geo DataFrame you should perform the following transformations:\n",
    "\n",
    "    Create a new column `coordinates` that contains an array based on the `latitude` and `longitude` columns.\\\n",
    "    Drop the `latitude` and `longitude` columns from the `DataFrame`. \\\n",
    "    Convert the `timestamp` column from a string to a `timestamp` data type. \\\n",
    "    Reorder the `DataFrame` columns to have the following column order:\\\n",
    "    `ind`\\\n",
    "    `country`\\\n",
    "    `coordinates`\\\n",
    "    `timestamp`\n",
    "    \"\"\"\n",
    "    ### Creates new column `coordinates` from `latitude` and `longitude` and drops them after creation\n",
    "    df_geo = df_geo.withColumn('coordinates', array(df_geo.latitude, df_geo.longitude)).drop('latitude','longitude')\n",
    "    ### Convert `timestamp` column to `timestamp` data \n",
    "    df_geo = df_geo.withColumn('timestamp', to_timestamp(df_geo.timestamp))\n",
    "    ### Reorder the columns as: `['ind', 'country', 'coordinates', 'timestamp']`\n",
    "    df_geo = df_geo[['ind', 'country', 'coordinates', 'timestamp']]\n",
    "\n",
    "    return df_geo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "76e69976-3dfe-48b5-93d7-322df751deb8",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/html": [
       "<style scoped>\n",
       "  .ansiout {\n",
       "    display: block;\n",
       "    unicode-bidi: embed;\n",
       "    white-space: pre-wrap;\n",
       "    word-wrap: break-word;\n",
       "    word-break: break-all;\n",
       "    font-family: \"Menlo\", \"Monaco\", \"Consolas\", \"Ubuntu Mono\", \"Source Code Pro\", monospace;\n",
       "    font-size: 13px;\n",
       "    color: #555;\n",
       "    margin-left: 4px;\n",
       "    line-height: 19px;\n",
       "  }\n",
       "</style>\n",
       "<div class=\"ansiout\"></div>"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "addedWidgets": {},
       "arguments": {},
       "data": "<div class=\"ansiout\"></div>",
       "datasetInfos": [],
       "metadata": {},
       "removedWidgets": [],
       "type": "html"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "def clean_user_dataframe(df_user):\n",
    "    \"\"\"\n",
    "    Clean User Spark DataFrame\n",
    "    To clean the df_user DataFrame you should perform the following transformations:\n",
    "\n",
    "    Create a new column `user_name` that concatenates the information found in the `first_name` and `last_name` columns. \\\n",
    "    Drop the `first_name` and `last_name` columns from the DataFrame. \\\n",
    "    Convert the `date_joined` column from a `string` to a `timestamp` data type. \\\n",
    "    Reorder the `DataFrame` columns to have the following column order: \\\n",
    "    `ind`, \n",
    "    `user_name`, \n",
    "    `age`, \n",
    "    `date_joined`\n",
    "    \"\"\"\n",
    "    df_user = df_user.withColumn('user_name', concat(df_user.first_name, df_user.last_name)).drop('first_name', 'last_name')\n",
    "    df_user = df_user.withColumn('date_joined', to_timestamp(df_user.date_joined))\n",
    "    df_user = df_user[['ind', 'user_name', 'age', 'date_joined']]\n",
    "\n",
    "    return df_user"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "889905a3-ea34-4a90-9ec9-f64680fb64c9",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "dashboards": [],
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "pinterest_data_cleaning",
   "widgets": {}
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
